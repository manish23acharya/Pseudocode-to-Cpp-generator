{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import torch\n", "import torch.nn as nn\n", "from torch.utils.data import Dataset, DataLoader\n", "import torch.optim as optim\n", "import torch.nn.functional as F\n", "import torchtext\n", "import matplotlib.pyplot as plt\n", "import matplotlib.ticker as ticker\n", "import spacy\n", "import numpy as np\n", "import pandas as pd\n", "import random\n", "import math\n", "import time\n", "from torch.utils.data import Dataset, DataLoader\n", "from tensorflow.keras.preprocessing.text import Tokenizer\n", "from tensorflow.keras.preprocessing.sequence import pad_sequences\n", "from sklearn.model_selection import train_test_split\n", "from tqdm import tqdm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Step 1: Load the Dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_df = pd.read_csv(\n", "    'C:\\\\Users\\\\Manish Acharya\\\\OneDrive\\\\Desktop\\\\Proj-2\\\\transformer\\\\spoc-train.tsv', delimiter='\\t')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Step 2: Handle Missing Values<br>\n", "Drop rows with missing values in 'text' or 'code'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_df = data_df.dropna(subset=['text', 'code'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Step 3: Tokenization and Vocabulary Creation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tokenizer = Tokenizer(filters='', lower=False, oov_token='<UNK>')\n", "combined_text = list(data_df['text']) + list(data_df['code'])\n", "tokenizer.fit_on_texts(combined_text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Vocabulary mapping: token to index"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vocab = tokenizer.word_index\n", "# Add special tokens to the vocabulary\n", "special_tokens = {'<PAD>': 0, '<SOS>': len(vocab) + 1, '<EOS>': len(vocab) + 2}\n", "vocab.update(special_tokens)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Padding Index"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["SRC_PAD_IDX = vocab['<PAD>']  # The padding index for input (pseudocode)\n", "TRG_PAD_IDX = vocab['<PAD>']  # The padding index for output (CPP code)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Reverse vocabulary mapping: index to token"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["reverse_vocab = {index: token for token, index in vocab.items()}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Input and Output Dimensions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["INPUT_DIM = len(vocab)  # Vocabulary size for pseudocode\n", "OUTPUT_DIM = len(vocab)  # Vocabulary size for CPP code"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Step 4: Numerical Representation and Padding"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pseudocode_sequences = tokenizer.texts_to_sequences(data_df['text'])\n", "cpp_code_sequences = tokenizer.texts_to_sequences(data_df['code'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Pad sequences to have the same length"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["max_sequence_length = max(len(seq)\n", "                          for seq in pseudocode_sequences + cpp_code_sequences)\n", "padded_pseudocode = pad_sequences(\n", "    pseudocode_sequences, maxlen=max_sequence_length, padding='post')\n", "padded_cpp_code = pad_sequences(\n", "    cpp_code_sequences, maxlen=max_sequence_length, padding='post')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Step 5: Data Splitting"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_pseudocode, val_test_pseudocode, train_cpp_code, val_test_cpp_code = train_test_split(\n", "    padded_pseudocode, padded_cpp_code, test_size=0.2, random_state=42\n", ")\n", "val_pseudocode, test_pseudocode, val_cpp_code, test_cpp_code = train_test_split(\n", "    val_test_pseudocode, val_test_cpp_code, test_size=0.5, random_state=42\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Step 6: Save Preprocessed Data and Vocabulary"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with open('vocab.txt', 'w') as file:\n", "    for token, index in vocab.items():\n", "        file.write(f\"{token}\\t{index}\\n\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.save('train_pseudocode.npy', train_pseudocode)\n", "np.save('val_pseudocode.npy', val_pseudocode)\n", "np.save('test_pseudocode.npy', test_pseudocode)\n", "np.save('train_cpp_code.npy', train_cpp_code)\n", "np.save('val_cpp_code.npy', val_cpp_code)\n", "np.save('test_cpp_code.npy', test_cpp_code)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check if GPU is available and use it, otherwise fall back to CPU"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "print(device)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Step 7: Define the Transformer Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Encoder(nn.Module):\n", "    def __init__(self,\n", "                 input_dim,\n", "                 hid_dim,\n", "                 n_layers,\n", "                 n_heads,\n", "                 pf_dim,\n", "                 dropout,\n", "                 device,\n", "                 max_length=100):\n", "        super().__init__()\n", "        self.device = device\n", "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n", "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n", "        self.layers = nn.ModuleList([EncoderLayer(hid_dim,\n", "                                                  n_heads,\n", "                                                  pf_dim,\n", "                                                  dropout,\n", "                                                  device)\n", "                                    for _ in range(n_layers)])\n", "        self.dropout = nn.Dropout(dropout)\n", "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n", "    def forward(self, src, src_mask):\n\n", "        # src = [batch size, src len]\n", "        # src_mask = [batch size, 1, 1, src len]\n", "        batch_size = src.shape[0]\n", "        src_len = src.shape[1]\n", "        pos = torch.arange(0, src_len).unsqueeze(\n", "            0).repeat(batch_size, 1).to(self.device)\n\n", "        # pos = [batch size, src len]\n", "        src = self.dropout(\n", "            (self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n\n", "        # src = [batch size, src len, hid dim]\n", "        for layer in self.layers:\n", "            src = layer(src, src_mask)\n\n", "            # src = [batch size, src len, hid dim]\n", "        return src"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class EncoderLayer(nn.Module):\n", "    def __init__(self,\n", "                 hid_dim,\n", "                 n_heads,\n", "                 pf_dim,\n", "                 dropout,\n", "                 device):\n", "        super().__init__()\n", "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n", "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n", "        self.self_attention = MultiHeadAttentionLayer(\n", "            hid_dim, n_heads, dropout, device)\n", "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim,\n", "                                                                     pf_dim,\n", "                                                                     dropout)\n", "        self.dropout = nn.Dropout(dropout)\n", "    def forward(self, src, src_mask):\n\n", "        # src = [batch size, src len, hid dim]\n", "        # src_mask = [batch size, 1, 1, src len]\n\n", "        # self attention\n", "        _src, _ = self.self_attention(src, src, src, src_mask)\n\n", "        # dropout, residual connection and layer norm\n", "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n\n", "        # src = [batch size, src len, hid dim]\n\n", "        # positionwise feedforward\n", "        _src = self.positionwise_feedforward(src)\n\n", "        # dropout, residual and layer norm\n", "        src = self.ff_layer_norm(src + self.dropout(_src))\n\n", "        # src = [batch size, src len, hid dim]\n", "        return src"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class PositionwiseFeedforwardLayer(nn.Module):\n", "    def __init__(self, hid_dim, pf_dim, dropout):\n", "        super().__init__()\n", "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n", "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n", "        self.dropout = nn.Dropout(dropout)\n", "    def forward(self, x):\n\n", "        # x = [batch size, seq len, hid dim]\n", "        x = self.dropout(torch.relu(self.fc_1(x)))\n\n", "        # x = [batch size, seq len, pf dim]\n", "        x = self.fc_2(x)\n\n", "        # x = [batch size, seq len, hid dim]\n", "        return x"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class MultiHeadAttentionLayer(nn.Module):\n", "    def __init__(self, hid_dim, n_heads, dropout, device):\n", "        super().__init__()\n", "        assert hid_dim % n_heads == 0\n", "        self.hid_dim = hid_dim\n", "        self.n_heads = n_heads\n", "        self.head_dim = hid_dim // n_heads\n", "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n", "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n", "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n", "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n", "        self.dropout = nn.Dropout(dropout)\n", "        self.scale = torch.sqrt(\n", "            torch.FloatTensor([self.head_dim])).to(device)\n", "    def forward(self, query, key, value, mask=None):\n", "        batch_size = query.shape[0]\n\n", "        # query = [batch size, query len, hid dim]\n", "        # key = [batch size, key len, hid dim]\n", "        # value = [batch size, value len, hid dim]\n", "        Q = self.fc_q(query)\n", "        K = self.fc_k(key)\n", "        V = self.fc_v(value)\n\n", "        # Q = [batch size, query len, hid dim]\n", "        # K = [batch size, key len, hid dim]\n", "        # V = [batch size, value len, hid dim]\n", "        Q = Q.view(batch_size, -1, self.n_heads,\n", "                   self.head_dim).permute(0, 2, 1, 3)\n", "        K = K.view(batch_size, -1, self.n_heads,\n", "                   self.head_dim).permute(0, 2, 1, 3)\n", "        V = V.view(batch_size, -1, self.n_heads,\n", "                   self.head_dim).permute(0, 2, 1, 3)\n\n", "        # Q = [batch size, n heads, query len, head dim]\n", "        # K = [batch size, n heads, key len, head dim]\n", "        # V = [batch size, n heads, value len, head dim]\n", "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n\n", "        # energy = [batch size, n heads, query len, key len]\n", "        if mask is not None:\n", "            energy = energy.masked_fill(mask == 0, -1e10)\n", "        attention = torch.softmax(energy, dim=-1)\n\n", "        # attention = [batch size, n heads, query len, key len]\n", "        x = torch.matmul(self.dropout(attention), V)\n\n", "        # x = [batch size, n heads, query len, head dim]\n", "        x = x.permute(0, 2, 1, 3).contiguous()\n\n", "        # x = [batch size, query len, n heads, head dim]\n", "        x = x.view(batch_size, -1, self.hid_dim)\n\n", "        # x = [batch size, query len, hid dim]\n", "        x = self.fc_o(x)\n\n", "        # x = [batch size, query len, hid dim]\n", "        return x, attention"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Decoder(nn.Module):\n", "    def __init__(self,\n", "                 output_dim,\n", "                 hid_dim,\n", "                 n_layers,\n", "                 n_heads,\n", "                 pf_dim,\n", "                 dropout,\n", "                 device,\n", "                 max_length=100):\n", "        super().__init__()\n", "        self.device = device\n", "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n", "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n", "        self.layers = nn.ModuleList([DecoderLayer(hid_dim,\n", "                                                  n_heads,\n", "                                                  pf_dim,\n", "                                                  dropout,\n", "                                                  device)\n", "                                     for _ in range(n_layers)])\n", "        self.fc_out = nn.Linear(hid_dim, output_dim)\n", "        self.dropout = nn.Dropout(dropout)\n", "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n", "    def forward(self, trg, enc_src, trg_mask, src_mask):\n\n", "        # trg = [batch size, trg len]\n", "        # enc_src = [batch size, src len, hid dim]\n", "        # trg_mask = [batch size, 1, trg len, trg len]\n", "        # src_mask = [batch size, 1, 1, src len]\n", "        batch_size = trg.shape[0]\n", "        trg_len = trg.shape[1]\n", "        pos = torch.arange(0, trg_len).unsqueeze(\n", "            0).repeat(batch_size, 1).to(self.device)\n\n", "        # pos = [batch size, trg len]\n", "        trg = self.dropout(\n", "            (self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n\n", "        # trg = [batch size, trg len, hid dim]\n", "        for layer in self.layers:\n", "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n\n", "        # trg = [batch size, trg len, hid dim]\n", "        # attention = [batch size, n heads, trg len, src len]\n", "        output = self.fc_out(trg)\n\n", "        # output = [batch size, trg len, output dim]\n", "        return output, attention"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DecoderLayer(nn.Module):\n", "    def __init__(self,\n", "                 hid_dim,\n", "                 n_heads,\n", "                 pf_dim,\n", "                 dropout,\n", "                 device):\n", "        super().__init__()\n", "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n", "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n", "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n", "        self.self_attention = MultiHeadAttentionLayer(\n", "            hid_dim, n_heads, dropout, device)\n", "        self.encoder_attention = MultiHeadAttentionLayer(\n", "            hid_dim, n_heads, dropout, device)\n", "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim,\n", "                                                                     pf_dim,\n", "                                                                     dropout)\n", "        self.dropout = nn.Dropout(dropout)\n", "    def forward(self, trg, enc_src, trg_mask, src_mask):\n\n", "        # trg = [batch size, trg len, hid dim]\n", "        # enc_src = [batch size, src len, hid dim]\n", "        # trg_mask = [batch size, 1, trg len, trg len]\n", "        # src_mask = [batch size, 1, 1, src len]\n\n", "        # self attention\n", "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n\n", "        # dropout, residual connection and layer norm\n", "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n\n", "        # trg = [batch size, trg len, hid dim]\n\n", "        # encoder attention\n", "        _trg, attention = self.encoder_attention(\n", "            trg, enc_src, enc_src, src_mask)\n", "        # query, key, value\n\n", "        # dropout, residual connection and layer norm\n", "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n\n", "        # trg = [batch size, trg len, hid dim]\n\n", "        # positionwise feedforward\n", "        _trg = self.positionwise_feedforward(trg)\n\n", "        # dropout, residual and layer norm\n", "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n\n", "        # trg = [batch size, trg len, hid dim]\n", "        # attention = [batch size, n heads, trg len, src len]\n", "        return trg, attention"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Seq2Seq(nn.Module):\n", "    def __init__(self,\n", "                 encoder,\n", "                 decoder,\n", "                 src_pad_idx,\n", "                 trg_pad_idx,\n", "                 device):\n", "        super().__init__()\n", "        self.encoder = encoder\n", "        self.decoder = decoder\n", "        self.src_pad_idx = src_pad_idx\n", "        self.trg_pad_idx = trg_pad_idx\n", "        self.device = device\n", "    def make_src_mask(self, src):\n\n", "        # src = [batch size, src len]\n", "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n\n", "        # src_mask = [batch size, 1, 1, src len]\n", "        return src_mask\n", "    def make_trg_mask(self, trg):\n\n", "        # trg = [batch size, trg len]\n", "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n\n", "        # trg_pad_mask = [batch size, 1, 1, trg len]\n", "        trg_len = trg.shape[1]\n", "        trg_sub_mask = torch.tril(torch.ones(\n", "            (trg_len, trg_len), device=self.device)).bool()\n\n", "        # trg_sub_mask = [trg len, trg len]\n", "        trg_mask = trg_pad_mask & trg_sub_mask\n\n", "        # trg_mask = [batch size, 1, trg len, trg len]\n", "        return trg_mask\n", "    def forward(self, src, trg):\n\n", "        # src = [batch size, src len]\n", "        # trg = [batch size, trg len]\n", "        src_mask = self.make_src_mask(src)\n", "        trg_mask = self.make_trg_mask(trg)\n\n", "        # src_mask = [batch size, 1, 1, src len]\n", "        # trg_mask = [batch size, 1, trg len, trg len]\n", "        enc_src = self.encoder(src, src_mask)\n\n", "        # enc_src = [batch size, src len, hid dim]\n", "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n\n", "        # output = [batch size, trg len, output dim]\n", "        # attention = [batch size, n heads, trg len, src len]\n", "        return output, attention"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["HID_DIM = 256\n", "ENC_LAYERS = 3\n", "DEC_LAYERS = 3\n", "ENC_HEADS = 16\n", "DEC_HEADS = 16\n", "ENC_PF_DIM = 512\n", "DEC_PF_DIM = 512\n", "ENC_DROPOUT = 0.1\n", "DEC_DROPOUT = 0.1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Build the Transformer Model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["enc = Encoder(INPUT_DIM, HID_DIM, ENC_LAYERS, ENC_HEADS,\n", "              ENC_PF_DIM, ENC_DROPOUT, device)\n", "dec = Decoder(OUTPUT_DIM, HID_DIM, DEC_LAYERS,\n", "              DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n", "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Step 8: Load Preprocessed Data and Vocabulary"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_pseudocode = np.load('train_pseudocode.npy')\n", "val_pseudocode = np.load('val_pseudocode.npy')\n", "test_pseudocode = np.load('test_pseudocode.npy')\n", "train_cpp_code = np.load('train_cpp_code.npy')\n", "val_cpp_code = np.load('val_cpp_code.npy')\n", "test_cpp_code = np.load('test_cpp_code.npy')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vocab = {}\n", "with open('vocab.txt', 'r') as file:\n", "    for line in file:\n", "        token, index = line.strip().split('\\t')\n", "        vocab[token] = int(index)\n", "vocab_size = len(vocab)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Add special tokens"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["PAD_token = vocab['<PAD>']\n", "SOS_token = vocab['<SOS>']\n", "EOS_token = vocab['<EOS>']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Step 9: Create PyTorch Dataset and DataLoader"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class CodeGenerationDataset(Dataset):\n", "    def __init__(self, pseudocode_sequences, cpp_code_sequences):\n", "        self.pseudocode_sequences = pseudocode_sequences\n", "        self.cpp_code_sequences = cpp_code_sequences\n", "    def __len__(self):\n", "        return len(self.pseudocode_sequences)\n", "    def __getitem__(self, index):\n", "        pseudocode = torch.tensor(\n", "            self.pseudocode_sequences[index], dtype=torch.long)\n", "        cpp_code = torch.tensor(\n", "            self.cpp_code_sequences[index], dtype=torch.long)\n", "        return pseudocode, cpp_code"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create datasets and data loaders"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["batch_size = 64\n", "train_dataset = CodeGenerationDataset(train_pseudocode, train_cpp_code)\n", "val_dataset = CodeGenerationDataset(val_pseudocode, val_cpp_code)\n", "test_dataset = CodeGenerationDataset(test_pseudocode, test_cpp_code)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n", "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n", "test_loader = DataLoader(test_dataset, batch_size=batch_size)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Step 10: \"Train the model\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define your model hyperparameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["INPUT_DIM = len(vocab)\n", "OUTPUT_DIM = len(vocab)\n", "HID_DIM = 256\n", "ENC_LAYERS = 3\n", "DEC_LAYERS = 3\n", "ENC_HEADS = 16\n", "DEC_HEADS = 16\n", "ENC_PF_DIM = 512\n", "DEC_PF_DIM = 512\n", "ENC_DROPOUT = 0.1\n", "DEC_DROPOUT = 0.1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create the encoder and decoder"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["enc = Encoder(INPUT_DIM, HID_DIM, ENC_LAYERS, ENC_HEADS,\n", "              ENC_PF_DIM, ENC_DROPOUT, device)\n", "dec = Decoder(OUTPUT_DIM, HID_DIM, DEC_LAYERS,\n", "              DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set the padding index for the source and target sequences"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["SRC_PAD_IDX = PAD_token\n", "TRG_PAD_IDX = PAD_token"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create the Seq2Seq model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Function to count trainable parameters in the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def count_parameters(model):\n", "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Print the number of trainable parameters in the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f'The model has {count_parameters(model):,} trainable parameters')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Function to initialize weights in the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def initialize_weights(m):\n", "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n", "        nn.init.xavier_uniform_(m.weight.data)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Apply weight initialization to the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def initialize_weights(m):\n", "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n", "        nn.init.xavier_uniform_(m.weight.data)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.apply(initialize_weights)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define the learning rate"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["LEARNING_RATE = 0.0005\n", "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define the optimizer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define the loss function (cross-entropy loss ignoring padding index)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class CrossEntropyLoss(nn.CrossEntropyLoss):\n", "    \"\"\"CrossEntropyLoss - with ability to receive distribution as targets, and optional label smoothing\"\"\"\n", "    def __init__(self, weight=None, ignore_index=-100, reduction='mean', smooth_eps=None, smooth_dist=None, from_logits=True):\n", "        super(CrossEntropyLoss, self).__init__(weight=weight,\n", "                                               ignore_index=ignore_index, reduction=reduction)\n", "        self.smooth_eps = smooth_eps\n", "        self.smooth_dist = smooth_dist\n", "        self.from_logits = from_logits\n", "    def forward(self, input, target, smooth_dist=None):\n", "        if smooth_dist is None:\n", "            smooth_dist = self.smooth_dist\n", "        return cross_entropy(input, target, weight=self.weight, ignore_index=self.ignore_index,\n", "                             reduction=self.reduction, smooth_eps=self.smooth_eps,\n", "                             smooth_dist=smooth_dist, from_logits=self.from_logits)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def cross_entropy(inputs, target, weight=None, ignore_index=-100, reduction='mean',\n", "                  smooth_eps=None, smooth_dist=None, from_logits=True):\n", " # \"\"\"Cross entropy loss, with support for target distributions and label smoothing https://arxiv.org/abs/1512.00567\"\"\"\n", "    smooth_eps = smooth_eps or 0\n\n", "    # Ordinary log-likelihood - use cross_entropy from nn\n", "    if _is_long(target) and smooth_eps == 0:\n", "        if from_logits:\n", "            return F.cross_entropy(inputs, target, weight, ignore_index=ignore_index, reduction=reduction)\n", "        else:\n", "            return F.nll_loss(inputs, target, weight, ignore_index=ignore_index, reduction=reduction)\n", "    if from_logits:\n", "        # Log-softmax of inputs\n", "        lsm = F.log_softmax(inputs, dim=-1)\n", "    else:\n", "        lsm = inputs\n", "    masked_indices = None\n", "    num_classes = inputs.size(-1)\n", "    if _is_long(target) and ignore_index >= 0:\n", "        masked_indices = target.eq(ignore_index)\n", "    if smooth_eps > 0 and smooth_dist is not None:\n", "        if _is_long(target):\n", "            target = onehot(target, num_classes).type_as(inputs)\n", "        if smooth_dist.dim() < target.dim():\n", "            smooth_dist = smooth_dist.unsqueeze(0)\n", "        target.lerp_(smooth_dist, smooth_eps)\n", "    if weight is not None:\n", "        lsm = lsm * weight.unsqueeze(0)\n", "    if _is_long(target):\n", "        eps_sum = smooth_eps / num_classes\n", "        eps_nll = 1. - eps_sum - smooth_eps\n", "        likelihood = lsm.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1)\n", "        loss = -(eps_nll * likelihood + eps_sum * lsm.sum(-1))\n", "    else:\n", "        loss = -(target * lsm).sum(-1)\n", "    if masked_indices is not None:\n", "        loss.masked_fill_(masked_indices, 0)\n", "    if reduction == 'sum':\n", "        loss = loss.sum()\n", "    elif reduction == 'mean':\n", "        if masked_indices is None:\n", "            loss = loss.mean()\n", "        else:\n", "            loss = loss.sum() / float(loss.size(0) - masked_indices.sum())\n", "    return loss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def onehot(indexes, N=None, ignore_index=None):\n", "    \"\"\"\n", "    Creates a one-representation of indexes with N possible entries\n", "    if N is not specified, it will suit the maximum index appearing.\n", "    indexes is a long-tensor of indexes\n", "    ignore_index will be zero in onehot representation\n", "    \"\"\"\n", "    if N is None:\n", "        N = indexes.max() + 1\n", "    sz = list(indexes.size())\n", "    output = indexes.new().byte().resize_(*sz, N).zero_()\n", "    output.scatter_(-1, indexes.unsqueeze(-1), 1)\n", "    if ignore_index is not None and ignore_index >= 0:\n", "        output.masked_fill_(indexes.eq(ignore_index).unsqueeze(-1), 0)\n", "    return output"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def _is_long(x):\n", "    if hasattr(x, 'data'):\n", "        x = x.data\n", "    return isinstance(x, torch.LongTensor) or isinstance(x, torch.cuda.LongTensor)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def maskNLLLoss(inp, target, mask):\n", "    nTotal = mask.sum()\n", "    crossEntropy = CrossEntropyLoss(ignore_index=TRG_PAD_IDX, smooth_eps=0.20)\n", "    loss = crossEntropy(inp, target)\n", "    loss = loss.to(device)\n", "    return loss, nTotal.item()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Replace TRG_PAD_IDX with the actual index of the padding token in your target vocabulary"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["TRG_PAD_IDX = vocab['<PAD>']  # The padding index for output (CPP code)\n", "criterion = maskNLLLoss"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Function to create the target mask"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def make_trg_mask(trg):\n", "    trg_pad_mask = (trg != TRG_PAD_IDX).unsqueeze(1).unsqueeze(2)\n", "    trg_len = trg.shape[1]\n", "    trg_sub_mask = torch.tril(torch.ones(\n", "        (trg_len, trg_len), device=device)).bool()\n", "    trg_mask = trg_pad_mask & trg_sub_mask\n", "    return trg_mask"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Your custom maskNLLLoss function<br>\n", "..."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Training function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train(model, iterator, optimizer, criterion, clip):\n", "    model.train()\n", "    n_totals = 0\n", "    print_losses = []\n", "    for batch in tqdm(iterator, total=len(iterator)):\n", "        src = batch[0].permute(1, 0)  # Access the input sequence\n", "        trg = batch[1].permute(1, 0)  # Access the output sequence\n", "        trg_mask = make_trg_mask(trg)\n", "        optimizer.zero_grad()\n", "        output, _ = model(src, trg[:, :-1])\n", "        output_dim = output.shape[-1]\n", "        output = output.contiguous().view(-1, output_dim)\n", "        trg = trg[:, 1:].contiguous().view(-1)\n", "        mask_loss, nTotal = criterion(output, trg, trg_mask)\n", "        mask_loss.backward()\n", "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n", "        optimizer.step()\n", "        print_losses.append(mask_loss.item() * nTotal)\n", "        n_totals += nTotal\n", "    return sum(print_losses) / n_totals"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate Function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate(model, iterator, criterion):\n", "    model.eval()\n", "    n_totals = 0\n", "    print_losses = []\n", "    with torch.no_grad():\n", "        for batch in tqdm(iterator, total=len(iterator)):\n", "            src = batch[0].permute(1, 0)  # Access the input sequence\n", "            trg = batch[1].permute(1, 0)  # Access the output sequence\n", "            trg_mask = make_trg_mask(trg)\n", "            output, _ = model(src, trg[:, :-1])\n", "            output_dim = output.shape[-1]\n", "            output = output.contiguous().view(-1, output_dim)\n", "            trg = trg[:, 1:].contiguous().view(-1)\n", "            mask_loss, nTotal = criterion(output, trg, trg_mask)\n", "            print_losses.append(mask_loss.item() * nTotal)\n", "            n_totals += nTotal\n", "    return sum(print_losses) / n_totals"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_path = os.path.join(os.path.dirname(\n", "    os.path.abspath(__file__)), 'model.pt')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Training Loop"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_checkpoint(model, optimizer, checkpoint_file):\n", "    checkpoint = torch.load(checkpoint_file)\n", "    model.load_state_dict(checkpoint['model_state_dict'])\n", "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n", "    epoch = checkpoint['epoch']\n", "    best_valid_loss = checkpoint['loss']\n", "    return epoch, best_valid_loss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def save_checkpoint(model, optimizer, epoch, loss, filename):\n", "    checkpoint = {\n", "        'epoch': epoch+1,\n", "        'model_state_dict': model.state_dict(),\n", "        'optimizer_state_dict': optimizer.state_dict(),\n", "        'loss': loss,\n", "    }\n\n", "    # After each epoch or a specific number of iterations\n", "    torch.save(checkpoint, 'checkpoint.pt')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["checkpoint_file = 'checkpoint.pt'\n", "if os.path.exists(checkpoint_file):\n", "    epoch, best_valid_loss = load_checkpoint(model, optimizer, checkpoint_file)\n", "    print(\n", "        f\"Resuming training from epoch {epoch + 1} with best validation loss: {best_valid_loss:.3f}\")\n", "else:\n", "    epoch = 0\n", "    best_valid_loss = float('inf')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def epoch_time(start_time, end_time):\n", "    elapsed_time = end_time - start_time\n", "    elapsed_mins = int(elapsed_time / 60)\n", "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n", "    return elapsed_mins, elapsed_secs"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["N_EPOCHS = 3\n", "CLIP = 1\n", "best_valid_loss = float('inf')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for epoch in range(N_EPOCHS):\n", "    start_time = time.time()\n", "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n", "    valid_loss = evaluate(model, val_loader, criterion)\n", "    end_time = time.time()\n", "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n", "    if valid_loss < best_valid_loss:\n", "        best_valid_loss = valid_loss\n", "        torch.save(\n", "            'https://drive.google.com/drive/u/0/folders/1ERMKFGQffkopZMlLtIUKDbHG6XhOfmdW', 'ourmodel.pt')\n", "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n", "    print(\n", "        f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n", "    print(\n", "        f'\\tVal. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f}')"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}